{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\python\\312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df['comment'] = df['comment'].str.replace('\\n', '', regex=False)  # Удаляем символы новой строки\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Токенизация\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, comments, labels):\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.comments[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Токенизация\n",
    "        inputs = tokenizer(comment, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Создание датасетов\n",
    "train_dataset = ToxicCommentsDataset(train_df['comment'].tolist(), train_df['toxic'].tolist())\n",
    "test_dataset = ToxicCommentsDataset(test_df['comment'].tolist(), test_df['toxic'].tolist())\n",
    "\n",
    "# Загрузка данных\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат прогноза текста \"Привет, это хороший день.\": 0\n"
     ]
    }
   ],
   "source": [
    "class PowerfulBinaryTextClassifier(nn.Module):\n",
    "    def __init__(self, model_name, lstm_hidden_size=256, num_layers=3, dropout_rate=0.2):\n",
    "        super(PowerfulBinaryTextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Добавляем несколько LSTM слоев с большим размером скрытого состояния\n",
    "        self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size, \n",
    "                            hidden_size=lstm_hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True, \n",
    "                            dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        # Полносвязный блок с увеличенным количеством нейронов и слоев Dropout\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_size * 2, 2),  # полносвязный слой\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)  # Инициализация токенизатора\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_outputs = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Применяем LSTM\n",
    "        lstm_out, _ = self.lstm(bert_outputs)  # (batch_size, sequence_length, lstm_hidden_size * 2)\n",
    "        \n",
    "        # Берем выход последнего временного шага для классификации\n",
    "        last_time_step = lstm_out[:, -1, :]  # (batch_size, lstm_hidden_size * 2)\n",
    "        \n",
    "        logits = self.fc(last_time_step)  # Применяем полносвязный блок\n",
    "        return logits  # Возвращаем логиты для двух классов\n",
    "\n",
    "    def predict(self, text):\n",
    "        self.to(self.device)  # Переносим модель на выбранное устройство\n",
    "        \n",
    "        # Токенизация текста\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        input_ids = inputs['input_ids'].to(self.device)  # Переносим на устройство\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)  # Переносим на устройство\n",
    "        \n",
    "        # Получение предсказания\n",
    "        self.eval()  # Переключаем модель в режим оценки\n",
    "        with torch.no_grad():\n",
    "            preds = self(input_ids, attention_mask)  # Получаем логиты\n",
    "        \n",
    "        # Возвращаем индекс класса с наибольшей вероятностью\n",
    "        return torch.argmax(preds, dim=1).item()  # Возвращаем индекс класса\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        # Загрузка весов модели\n",
    "        self.load_state_dict(torch.load(filepath, map_location=self.device))\n",
    "\n",
    "# Пример инициализации модели\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "model = PowerfulBinaryTextClassifier(model_name)\n",
    "\n",
    "# Пример использования\n",
    "text = \"Привет, это хороший день.\"\n",
    "predicted_class_index = model.predict(text)\n",
    "print(f'Результат прогноза текста \"{text}\": {predicted_class_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1 Точность на тестовых данных: 0.7787, F1-мера: 0.7438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 2 Точность на тестовых данных: 0.9136, F1-мера: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 3 Точность на тестовых данных: 0.9126, F1-мера: 0.9132\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # Для отображения прогресса обучения\n",
    "from sklearn.metrics import accuracy_score, f1_score  # Импортируем метрики\n",
    "\n",
    "# Установка параметров обучения\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = PowerfulBinaryTextClassifier(model_name).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Функция потерь для многоклассовой классификации\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Переключаем модель в режим оценки\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Отключаем подсчет градиентов для оценки\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Получаем предсказания\n",
    "\n",
    "            # Сохраняем предсказания и истинные метки\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Вычисляем метрики\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f'Точность на тестовых данных: {accuracy:.4f}, F1-мера: {f1:.4f}')\n",
    "    \n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Переключаем модель в режим обучения\n",
    "        total_loss = 0\n",
    "        all_labels = []  # Для хранения всех меток\n",
    "        all_preds = []   # Для хранения всех предсказаний\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Эпоха {epoch+1}/{num_epochs}', leave=False):\n",
    "            optimizer.zero_grad()  # Обнуляем градиенты\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels.long())  # Применяем функцию потерь\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Обратный проход и обновление параметров\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Получаем предсказания\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Предсказания\n",
    "            all_preds.extend(preds)  # Добавляем предсказания в общий список\n",
    "            all_labels.extend(labels.cpu().numpy())  # Добавляем истинные метки в общий список\n",
    "        \n",
    "        print(f'Эпоха {epoch+1}', end=' ')\n",
    "        evaluate_model(model, test_loader)\n",
    "\n",
    "# Запуск процесса обучения\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Сохранение модели после обучения\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: \"Ты в своем уме? Как можно быть таким тупым и неадекватным?\" Класс: 1\n",
      "Текст: \"Этот бред просто поражает, как можно так не понимать простые вещи?\" Класс: 0\n",
      "Текст: \"Как же ты надоел своими безумными идеями, убирайся к черту!\" Класс: 1\n",
      "Текст: \"Сколько можно тратить время на твои идиотские комментарии?\" Класс: 1\n",
      "Текст: \"Кто тебе дал право говорить такие глупости? Умоляю, замолчи!\" Класс: 1\n",
      "Текст: \"Ты как всегда на высоте — высоте своего безумия.\" Класс: 1\n",
      "Текст: \"Ужас, просто ужас. Надеюсь, ты хоть сам понимаешь, как ты выглядишь.\" Класс: 1\n",
      "Текст: \"Почему ты не можешь оставить свои бредовые мысли при себе?\" Класс: 1\n",
      "Текст: \"Твои слова — это просто смех и позор, не позорься больше!\" Класс: 1\n",
      "Текст: \"Слушай, может, тебе стоит просто уйти и больше не возвращаться?\" Класс: 1\n",
      "Текст: \"Недавно посмотрел интересный фильм — действительно затянуло!\" Класс: 0\n",
      "Текст: \"Каждый день пробую новые рецепты — это так увлекательно.\" Класс: 0\n",
      "Текст: \"Недавно начал читать новую книгу, и она просто захватывающая.\" Класс: 0\n",
      "Текст: \"Погода сегодня прекрасная, надеюсь, выйду на прогулку!\" Класс: 0\n",
      "Текст: \"Вчера попробовал новый кофе — он оказался невероятно вкусным.\" Класс: 0\n",
      "Текст: \"В выходные собираюсь съездить на природу, очень жду этого!\" Класс: 0\n",
      "Текст: \"Занимаюсь спортом, и это приносит много радости в жизнь.\" Класс: 0\n",
      "Текст: \"Посмотрел новый сериал, и он оказался очень увлекательным.\" Класс: 0\n",
      "Текст: \"В последнее время увлекся рисованием — это очень расслабляет.\" Класс: 0\n",
      "Текст: \"На выходных встретился с друзьями — было действительно весело!\" Класс: 0\n"
     ]
    }
   ],
   "source": [
    "# Пример использования \n",
    "comments = [\n",
    "    # Токсичные комментарии (Класс: 1)\n",
    "    \"Ты в своем уме? Как можно быть таким тупым и неадекватным?\",\n",
    "    \"Этот бред просто поражает, как можно так не понимать простые вещи?\",\n",
    "    \"Как же ты надоел своими безумными идеями, убирайся к черту!\",\n",
    "    \"Сколько можно тратить время на твои идиотские комментарии?\",\n",
    "    \"Кто тебе дал право говорить такие глупости? Умоляю, замолчи!\",\n",
    "    \"Ты как всегда на высоте — высоте своего безумия.\",\n",
    "    \"Ужас, просто ужас. Надеюсь, ты хоть сам понимаешь, как ты выглядишь.\",\n",
    "    \"Почему ты не можешь оставить свои бредовые мысли при себе?\",\n",
    "    \"Твои слова — это просто смех и позор, не позорься больше!\",\n",
    "    \"Слушай, может, тебе стоит просто уйти и больше не возвращаться?\",\n",
    "\n",
    "    # Нетоксичные комментарии (Класс: 0)\n",
    "    \"Недавно посмотрел интересный фильм — действительно затянуло!\",\n",
    "    \"Каждый день пробую новые рецепты — это так увлекательно.\",\n",
    "    \"Недавно начал читать новую книгу, и она просто захватывающая.\",\n",
    "    \"Погода сегодня прекрасная, надеюсь, выйду на прогулку!\",\n",
    "    \"Вчера попробовал новый кофе — он оказался невероятно вкусным.\",\n",
    "    \"В выходные собираюсь съездить на природу, очень жду этого!\",\n",
    "    \"Занимаюсь спортом, и это приносит много радости в жизнь.\",\n",
    "    \"Посмотрел новый сериал, и он оказался очень увлекательным.\",\n",
    "    \"В последнее время увлекся рисованием — это очень расслабляет.\",\n",
    "    \"На выходных встретился с друзьями — было действительно весело!\"\n",
    "]\n",
    "\n",
    "for text in comments:\n",
    "    predicted_class_index = model.predict(text) # прогноз класса текста\n",
    "    print(f'Текст: \"{text}\" Класс: {predicted_class_index}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
